{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"natural_language_processing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMx/KsxUDrn2M5QbIb03B9p"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VwK5-9FIB-lu","colab_type":"text"},"source":["# Natural Language Processing"]},{"cell_type":"markdown","metadata":{"id":"X1kiO9kACE6s","colab_type":"text"},"source":["## Importing the libraries"]},{"cell_type":"code","metadata":{"id":"7QG7sxmoCIvN","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wTfaCIzdCLPA","colab_type":"text"},"source":["## Importing the dataset"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# delimiter is used to read tsv files, quoing is used to ignore quotes \n","dataset = pd.read_csv('Restaurant_Reviews.tsv',delimiter= '\\t',quoting=3)"]},{"cell_type":"markdown","metadata":{"id":"Qekztq71CixT","colab_type":"text"},"source":["## Cleaning the texts"]},{"cell_type":"code","execution_count":22,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\jainh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"}],"source":["# stopwords are words which give no meaning(like a,the,etc)\n","# stem would help to get the root of the word(loved => love)\n","# corpus would contain cleant reviews\n","import re\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","corpus = []\n","for i in range(0,1000):\n","    review = re.sub('[^a-zA-Z]',' ',dataset['Review'][i])\n","    review = review.lower()\n","    review = review.split()\n","    ps = PorterStemmer()\n","    all_stopwords = stopwords.words('english')\n","    all_stopwords.remove('not')\n","    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n","    review = ' '.join(review)\n","    corpus.append(review)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CLqmAkANCp1-","colab_type":"text"},"source":["## Creating the Bag of Words model"]},{"cell_type":"code","execution_count":23,"metadata":{"tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":"1500"},"metadata":{},"execution_count":23}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","cv = CountVectorizer(max_features=1500)\n","X = cv.fit_transform(corpus).toarray()\n","y = dataset.iloc[:,-1].values\n","len(X[0])"]},{"cell_type":"markdown","metadata":{"id":"DH_VjgPzC2cd","colab_type":"text"},"source":["## Splitting the dataset into the Training set and Test set"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"]},{"cell_type":"markdown","metadata":{"id":"VkIq23vEDIPt","colab_type":"text"},"source":["## Training the Naive Bayes model on the Training set"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"GaussianNB(priors=None, var_smoothing=1e-09)"},"metadata":{},"execution_count":25}],"source":["from sklearn.naive_bayes import GaussianNB\n","classifier = GaussianNB()\n","classifier.fit(X_train,y_train)"]},{"cell_type":"markdown","metadata":{"id":"1JaRM7zXDWUy","colab_type":"text"},"source":["## Predicting the Test set results"]},{"cell_type":"code","execution_count":26,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"[[1 0]\n [1 0]\n [1 0]\n [0 0]\n [0 0]\n [1 0]\n [1 1]\n [1 0]\n [1 0]\n [1 1]\n [1 1]\n [1 1]\n [1 0]\n [1 1]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 1]\n [1 1]\n [1 0]\n [1 0]\n [0 1]\n [1 1]\n [1 1]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [0 0]\n [1 0]\n [0 0]\n [1 0]\n [1 1]\n [1 1]\n [1 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [1 0]\n [1 0]\n [0 0]\n [0 0]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [1 0]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [1 1]\n [1 0]\n [0 0]\n [1 0]\n [1 0]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [1 1]\n [1 0]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [0 0]\n [1 0]\n [1 1]\n [0 1]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [1 0]\n [0 0]\n [1 1]\n [1 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [1 0]\n [1 1]\n [1 0]\n [1 1]\n [1 1]\n [1 0]\n [0 1]\n [1 1]\n [1 1]\n [1 0]\n [0 1]\n [1 0]\n [1 1]\n [1 1]\n [0 0]\n [0 1]\n [0 1]\n [1 1]\n [0 0]\n [1 0]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [0 0]\n [1 1]\n [1 0]\n [0 0]\n [0 0]\n [1 1]\n [1 0]\n [0 0]\n [1 1]\n [1 0]\n [1 1]\n [0 0]\n [0 0]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [1 0]\n [0 1]\n [1 1]\n [1 1]\n [0 0]\n [1 0]\n [0 0]\n [1 0]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [0 1]\n [1 1]\n [1 1]\n [1 0]\n [0 0]\n [1 1]\n [1 1]\n [1 1]\n [1 0]\n [1 0]\n [0 0]\n [0 1]\n [1 1]\n [0 0]\n [0 0]\n [1 0]\n [0 0]\n [0 0]\n [0 1]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [0 1]\n [1 1]\n [0 0]\n [0 0]\n [1 0]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [1 0]\n [1 0]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [1 0]\n [1 1]]\n"}],"source":["y_pred = classifier.predict(X_test)\n","print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"]},{"cell_type":"markdown","metadata":{"id":"xoMltea5Dir1","colab_type":"text"},"source":["## Making the Confusion Matrix"]},{"cell_type":"code","execution_count":19,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"[[ 67  50]\n [ 20 113]]\n"},{"output_type":"execute_result","data":{"text/plain":"0.72"},"metadata":{},"execution_count":19}],"source":["from sklearn.metrics import confusion_matrix, accuracy_score\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)\n","accuracy_score(y_test, y_pred)"]}]}